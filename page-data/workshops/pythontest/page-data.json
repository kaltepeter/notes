{"componentChunkName":"component---src-templates-note-template-tsx","path":"/workshops/pythontest/","result":{"data":{"markdownRemark":{"html":"<p><a href=\"https://pythontest.teachable.com/p/complete-pytest-course\">https://pythontest.teachable.com/p/complete-pytest-course</a></p>\n<p>The videos are more up to date than the book(<a href=\"https://pragprog.com/titles/bopytest2/python-testing-with-pytest-second-edition/\">https://pragprog.com/titles/bopytest2/python-testing-with-pytest-second-edition/</a>). The video matches the book chapters.</p>\n<p>The book on oreilly: <a href=\"https://learning.oreilly.com/library/view/python-testing-with/9781680509427/\">https://learning.oreilly.com/library/view/python-testing-with/9781680509427/</a></p>\n<h2>Running</h2>\n<p><code>-v</code> verbose, add more v's for more detail</p>\n<p><code>pytest</code> basic run</p>\n<p><code>pytest &#x3C;path></code> execute tests in a dir</p>\n<p><code>--tb=no</code> disable traceback</p>\n<p><code>pytest -v --tb=no --no-summary</code> concise run output</p>\n<p><code>pytest -v --tb=no --no-summary -k _classes</code> run all with keyword 'classes'</p>\n<p><code>pytest -v --tb=no --no-summary -k \"equality and diff\"</code> logic for keyword equality and not diff</p>\n<p><code>-s or --capture=no</code> turns off pytest output capture. By default passing tests don't log anything, passing <code>-s</code> will log output.</p>\n<p><code>--setup-show</code> show test setup</p>\n<p>VSCode python extension supports running tests, they need to be configured. <a href=\"https://code.visualstudio.com/docs/python/testing\">https://code.visualstudio.com/docs/python/testing</a></p>\n<h2>Test Discovery</h2>\n<ul>\n<li>Files named test_*.py or *_test.py</li>\n<li>Classes names TestSomething</li>\n</ul>\n<h2>Test Outcomes</h2>\n<ul>\n<li>PASSED (.)</li>\n<li>FAILED (F) - exception in test</li>\n<li>SKIPPED (S) - marked to skip</li>\n<li>XFAIL (x) - marked as expected to fail</li>\n<li>XPASS (X) - expected to fail but passed</li>\n<li>ERROR (E) - exception in fixture</li>\n</ul>\n<p><code>@pytest.mark.xfail(reason=\"example of xfail\")</code> will output 'example of xfail' in the log</p>\n<h2>Pip</h2>\n<p>get pip deps</p>\n<pre><code class=\"language-python\">pip install pipdeptree\npipdeptree\n\npip list --not-required # show top level items\n</code></pre>\n<h2>Writing Test Functions</h2>\n<pre><code class=\"language-python\">assert c is None # checking none\nassert c == 0   # checking equality\n\n# code in block raises exception\n# This is preferred\nwith pytest.raises(TypeError):\n    pass\n\n# If you need more.\n# more specific exception checking, messy, use the above style more often\nwith pytest.raises(TypeError) as exc_info:\n    cards.CardsDB()\n    expected = \"missing 1 required positional argument\"\n    assert expected in str(exc_info.value)\n</code></pre>\n<h2>Assertion Helpers</h2>\n<p>Error Messages</p>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 650px; \"\n    >\n      <a\n    class=\"gatsby-resp-image-link\"\n    href=\"/notes/static/8c1817e093b7d2a0ba4ec4495b44202a/51dd6/pytest-with-traceback.png\"\n    style=\"display: block\"\n    target=\"_blank\"\n    rel=\"noopener\"\n  >\n    <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 54.601226993865026%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAALCAYAAAB/Ca1DAAAACXBIWXMAABYlAAAWJQFJUiTwAAABhUlEQVR42p1S2VLCQBDMT2g4EshuLhJyCYQjGA6BcKil/v/HtDMLaKkVHnzomsnsbs90T7Q8m+C03qPavWNbfWBXvaF6OuK0OuB1fcDzck+4RKq9XHAod9jPN9gWG1QUj4sK5WQJLewleJwuUc5WKIs15SuVF1SbUz6nyHkxWWA2Ls+RMB4UyAczjLIphgT+TuIcWqvr4K5twfIiBGkOr/8APxqiYUrcGwJ6Heic0bhANwUM4UMzLBfCDeH0YkKicoOatDoSTdNGy+Qo0abaFzpU/wOJjiRCU3iKxKfJ3CCF5QQUE0g/gk1NbJrc9mNFxM3bqtkNQr7EJEE8RJjk8MMMQTJS0j3KLWomvX4NSQ0hF/hxlI2JeKAa8ORc18nfhmEp2VfcJOzagZLBhAltjafs03J60UB56vRSJdkmC3hS9vo6RI2HvvJQklfsoUskYTo6y3VCRcpEfC7cviK+SXjd3G/Dm3RBJ6ksWUH9Lixf/JD/DQEeTrOItcuLsbwzxP8hnQCfab9AOuuOmOEAAAAASUVORK5CYII='); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"With Traceback\"\n        title=\"\"\n        src=\"/notes/static/8c1817e093b7d2a0ba4ec4495b44202a/a6d36/pytest-with-traceback.png\"\n        srcset=\"/notes/static/8c1817e093b7d2a0ba4ec4495b44202a/222b7/pytest-with-traceback.png 163w,\n/notes/static/8c1817e093b7d2a0ba4ec4495b44202a/ff46a/pytest-with-traceback.png 325w,\n/notes/static/8c1817e093b7d2a0ba4ec4495b44202a/a6d36/pytest-with-traceback.png 650w,\n/notes/static/8c1817e093b7d2a0ba4ec4495b44202a/e548f/pytest-with-traceback.png 975w,\n/notes/static/8c1817e093b7d2a0ba4ec4495b44202a/3c492/pytest-with-traceback.png 1300w,\n/notes/static/8c1817e093b7d2a0ba4ec4495b44202a/51dd6/pytest-with-traceback.png 1952w\"\n        sizes=\"(max-width: 650px) 100vw, 650px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n        decoding=\"async\"\n      />\n  </a>\n    </span></p>\n<p><code>__tracebackhide__ = True</code> hide traceback for helper funtions, makes it clearer where the failure happened.</p>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 650px; \"\n    >\n      <a\n    class=\"gatsby-resp-image-link\"\n    href=\"/notes/static/90ed6b5e42a664a8069d186b6566de55/df88b/pytest-hidden-traceback.png\"\n    style=\"display: block\"\n    target=\"_blank\"\n    rel=\"noopener\"\n  >\n    <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 31.288343558282207%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAGCAYAAADDl76dAAAACXBIWXMAABYlAAAWJQFJUiTwAAAA6ElEQVR42p2Q626DMAxGeYuuK9dcSGgDHXSoUwNMLZe+/xN9c1IxqdqkSftx5NiJjxwHjWlwHxbM/Yzlc/HMw4ypn3wcKa5c7Q1jN+FmRwznAV3bw753sG3n83NzQbCNOV4TiVQaKHOCKN4g9zU2IXsm4t/nFzpvY+H7VlweZgpBlOVwaFOjbi2q5gPH0wVFSXJdQuoKPDcQqkQm974WpiRxgzzByENCd5nwgh4fIIuKOHqZplW4miAhyw/gynghV38IdzRuzLQnYorGfky8o6ZNmP2KW9NP2OPLCTVGVFiJE/FPOFKm8AXEJa4TKCv2VgAAAABJRU5ErkJggg=='); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"Hidden Traceback\"\n        title=\"\"\n        src=\"/notes/static/90ed6b5e42a664a8069d186b6566de55/a6d36/pytest-hidden-traceback.png\"\n        srcset=\"/notes/static/90ed6b5e42a664a8069d186b6566de55/222b7/pytest-hidden-traceback.png 163w,\n/notes/static/90ed6b5e42a664a8069d186b6566de55/ff46a/pytest-hidden-traceback.png 325w,\n/notes/static/90ed6b5e42a664a8069d186b6566de55/a6d36/pytest-hidden-traceback.png 650w,\n/notes/static/90ed6b5e42a664a8069d186b6566de55/e548f/pytest-hidden-traceback.png 975w,\n/notes/static/90ed6b5e42a664a8069d186b6566de55/3c492/pytest-hidden-traceback.png 1300w,\n/notes/static/90ed6b5e42a664a8069d186b6566de55/df88b/pytest-hidden-traceback.png 1906w\"\n        sizes=\"(max-width: 650px) 100vw, 650px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n        decoding=\"async\"\n      />\n  </a>\n    </span></p>\n<h2>Structure</h2>\n<p>Given/When/Then or Arrange/Act/Assert</p>\n<p>Grouping in classes</p>\n<pre><code class=\"language-python\">class TestEquality:\n    def test_equality(self):\n        c1 = Card(\"something\", \"brian\", \"todo\", 123)\n        c2 = Card(\"something\", \"brian\", \"todo\", 123)\n        assert c1 == c2\n</code></pre>\n<p>Running just the class: <code>pytest test_classes.py::TestEquality -v</code></p>\n<h2>Pytest Fixtures</h2>\n<p><code>@pytest.fixture()</code> creates a fixture</p>\n<pre><code class=\"language-python\">@pytest.fixture()\ndef fixture_name()\n    # arrange/setup\n    yield # or return if no teardown\n    # teardown\n\n# passin to test\ndef test_something(fixture_name):\n    # act\n    # assert\n</code></pre>\n<p><code>pytest test_count.py -v --setup-show</code> will trace control flow in tests.</p>\n<h3>Scopes</h3>\n<p><code>@pytest.fixture(scope=\"module\")</code> creates a fixture in the module scope. This can create one setup/teardown for the module, like expensive db setup.</p>\n<ul>\n<li><code>scope=\"function\"</code> (default) - run once per test function</li>\n<li><code>scope='class'</code> - run once per test class</li>\n<li><code>scope='module'</code> - run once per module</li>\n<li><code>scope='package'</code> - run once per test directory</li>\n<li><code>scope='session'</code> - run once per test session (invocation of pytest)</li>\n</ul>\n<h3>Sharing Fixtures Through <code>conftest.py</code></h3>\n<ul>\n<li>Moving fixtures to <code>conftest.py</code> will make them available to multiple files without imports.</li>\n<li>You can have multiple <code>conftest.py</code> files.</li>\n<li>Run with <code>--fixures</code> to find all fixtures.</li>\n<li><code>pytest a --fixtures-per-test</code> list all fixutres per test for the directory <code>a</code></li>\n</ul>\n<h3>Multiple Fixture Levels</h3>\n<ul>\n<li>Fixtures at a higher level, like session can preserve state which will break or make tests less reliable.</li>\n<li>You can layer fixtures to handle things seperately, like session scope is just creating db, and the function scope can use that fixure to clear the data.</li>\n</ul>\n<pre><code class=\"language-python\">@pytest.fixture(scope=\"session\")\ndef db():\n    \"\"\"CardsDB object connected to a temporary database\"\"\"\n    with TemporaryDirectory() as db_dir:\n        db_path = Path(db_dir)\n        db_ = cards.CardsDB(db_path)\n        yield db_\n        db_.close()\n\n\n@pytest.fixture(scope=\"function\")\ndef cards_db(db):\n    \"\"\"CardsDB object that's empty\"\"\"\n    db.delete_all()\n    return db\n</code></pre>\n<h3>Dynamic Scope</h3>\n<p>This will change db scope based on command line arg of <code>--func-db</code>. To run with a clean db after each function pass that in.</p>\n<p><code>pytest d/test_count.py --setup-show --func-db</code></p>\n<pre><code class=\"language-python\">def pytest_addoption(parser):\n    parser.addoption(\n        \"--func-db\",\n        action=\"store_true\",\n        default=False,\n        help=\"new db for each test\",\n    )\n\n\ndef db_scope(fixture_name, config):\n    if config.getoption(\"--func-db\", None):\n        return \"function\"\n    return \"session\"\n\n\n@pytest.fixture(scope=db_scope)\ndef db():\n    \"\"\"CardsDB object connected to a temporary database\"\"\"\n    with TemporaryDirectory() as db_dir:\n        db_path = Path(db_dir)\n        db_ = cards.CardsDB(db_path)\n        yield db_\n        db_.close()\n</code></pre>\n<h3>Autouse</h3>\n<p>Example, wrapping with a time duration output. Autouse doesn't have a lot of use cases but is powerful.</p>\n<pre><code class=\"language-python\">@pytest.fixture(autouse=True, scope=\"session\")\ndef footer_session_scope():\n    \"\"\"Report the time at the end of a session.\"\"\"\n    yield\n    now = time.time()\n    print(\"--\")\n    print(\"finished : {}\".format(\n        time.strftime(\"%d %b %X\", time.localtime(now))))\n    print(\"-----------------\")\n\n\n@pytest.fixture(autouse=True)\ndef footer_function_scope():\n    \"\"\"Report test durations after each function.\"\"\"\n    start = time.time()\n    yield\n    stop = time.time()\n    delta = stop - start\n    print(\"\\ntest duration : {:0.3} seconds\".format(delta))\n</code></pre>\n<h3>Rename Fixtures</h3>\n<p>Use case: obvious name is taken by something already imported.</p>\n<pre><code class=\"language-python\">@pytest.fixture(name=\"ultimate_answer\")\ndef ultimate_answer_fixture():\n    return 42\n\n\ndef test_everything(ultimate_answer):\n    assert ultimate_answer == 42\n</code></pre>\n<h2>Builtin Fixtures</h2>\n<p><code>pytest --fixtures</code> to list them, lists the builtins</p>\n<p><code>tmp_path_factory</code> [session scope] -- .../_pytest/tmpdir.py:241\nReturn a :class:<code>pytest.TempPathFactory</code> instance for the test session.</p>\n<p><code>tmp_path</code> -- .../_pytest/tmpdir.py:256\nReturn a temporary directory path object which is unique to each test\nfunction invocation, created as a sub directory of the base temporary\ndirectory.</p>\n<p><code>tmp_path_retention_count</code> (default 3) controls how many tmp files are preserved for debugging based on policy, default policy <code>all</code>. <a href=\"https://docs.pytest.org/en/latest/reference/reference.html#confval-tmp_path_retention_count\">https://docs.pytest.org/en/latest/reference/reference.html#confval-tmp_path_retention_count</a></p>\n<pre><code class=\"language-python\">def test_tmp_path(tmp_path):\n    file = tmp_path / \"file.txt\"\n    file.write_text(\"Hello\")\n    assert file.read_text() == \"Hello\"\n\n\ndef test_tmp_path_factory(tmp_path_factory):\n    path = tmp_path_factory.mktemp(\"sub\")\n    file = path / \"file.txt\"\n    file.write_text(\"Hello\")\n    assert file.read_text() == \"Hello\"\n</code></pre>\n<p><code>capsys</code> -- .../_pytest/capture.py:981\nEnable text capturing of writes to <code>sys.stdout</code> and <code>sys.stderr</code>.</p>\n<p>Captures output during test execution, then provides methods to access the captured output.</p>\n<pre><code class=\"language-python\"># manual way\ndef test_version_v1():\n    process = subprocess.run(\n        [\"cards\", \"version\"], capture_output=True, text=True\n    )\n    output = process.stdout.rstrip()\n    assert output == cards.__version__\n\n# capsys\ndef test_version_v2(capsys):\n    cards.cli.version()\n    output = capsys.readouterr().out.rstrip()\n    assert output == cards.__version__\n\n# print all output, not just failing, use this for particular tests\ndef test_disabled(capsys):\n    with capsys.disabled():\n        print(\"\\ncapsys disabled print\")\n</code></pre>\n<p>Typer: used for buildling CLIs and has testing utils. Capsys is not needed typically with this.</p>\n<pre><code class=\"language-python\">import cards\nfrom typer.testing import CliRunner\n\n\ndef test_version_v3():\n    runner = CliRunner()\n    result = runner.invoke(cards.app, [\"version\"])\n    output = result.output.rstrip()\n    assert output == cards.__version__\n</code></pre>\n<h3>Monkey Patching</h3>\n<p><a href=\"https://docs.pytest.org/en/latest/how-to/monkeypatch.html\">https://docs.pytest.org/en/latest/how-to/monkeypatch.html</a></p>\n<p><code>monkeypatch</code> -- .../_pytest/monkeypatch.py:32\nA convenient fixture for monkey-patching.</p>\n<p><code>cards.cli.pathlib.Path</code> is patching the pathlib.Path import in cards.cli</p>\n<pre><code class=\"language-python\"># source\ndef get_path():\n    db_path_env = os.getenv(\"CARDS_DB_DIR\", \"\")\n    if db_path_env:\n        db_path = pathlib.Path(db_path_env)\n    else:\n        db_path = pathlib.Path.home() / \"cards_db\"\n    return db_path\n\n# patch\ndef test_patch_get_path(monkeypatch, tmp_path):\n    def fake_get_path():\n        return tmp_path\n\n    monkeypatch.setattr(cards.cli,\n                        \"get_path\",\n                        fake_get_path)\n    assert run_cards(\"config\") == str(tmp_path)\n    assert run_cards(\"count\") == \"0\"\n\n# source\nPath.home() # patch only home\n\n# patch\ndef test_patch_home(monkeypatch, tmp_path):\n    def fake_home():\n        return tmp_path\n\n    monkeypatch.setattr(cards.cli.pathlib.Path,\n                        \"home\",\n                        fake_home)\n\n    full_cards_dir = tmp_path / \"cards_db\"\n    print(full_cards_dir)\n    assert run_cards(\"config\") == str(full_cards_dir)\n    assert run_cards(\"count\") == \"0\"\n\n# source\nos.getenv(\"CARDS_DB_DIR\", \"\") # only the env var\n\n# patch only env var\ndef test_patch_env_var(monkeypatch, tmp_path):\n    monkeypatch.setenv(\"CARDS_DB_DIR\", str(tmp_path))\n\n    assert run_cards(\"config\") == str(tmp_path)\n    assert run_cards(\"count\") == \"0\"\n</code></pre>\n<ul>\n<li>Patch the smallest amount as possible.</li>\n<li>Design for testability, i.e. env var</li>\n</ul>\n<h2>Parameterization</h2>\n<p>Turning one test funciton into multiple test cases.</p>\n<ul>\n<li>A test failure is easier to debug than a loop</li>\n<li>Reduces boilerplate</li>\n<li>Only parameterize the meaningful changes</li>\n<li>Parameterize the fixture to make it easier to do work around the params. Returning the param makes the tests nice.</li>\n</ul>\n<pre><code class=\"language-python\"># no parameters\ndef test_finish_from_done(cards_db):\n    index = cards_db.add_card(Card(\"write pytest book\", state=\"done\"))\n    cards_db.finish(index)\n    card = cards_db.get_card(index)\n    assert card.state == \"done\"\n\n\ndef test_finish_from_in_prog(cards_db):\n    index = cards_db.add_card(Card(\"create video course\", state=\"in prog\"))\n    cards_db.finish(index)\n    card = cards_db.get_card(index)\n    assert card.state == \"done\"\n\n\ndef test_finish_from_todo(cards_db):\n    index = cards_db.add_card(Card(\"write tdd book\", state=\"todo\"))\n    cards_db.finish(index)\n    card = cards_db.get_card(index)\n    assert card.state == \"done\"\n\n# with loop\ndef test_finish(cards_db):\n    for c in [\n        Card(\"write pytest book\", state=\"done\"),\n        Card(\"create video course\", state=\"in prog\"),\n        Card(\"write tdd book\", state=\"todo\"),\n    ]:\n        index = cards_db.add_card(c)\n        cards_db.finish(index)\n        card = cards_db.get_card(index)\n        assert card.state == \"done\"\n\n# with parameters\n@pytest.mark.parametrize(\n    \"start_summary, start_state\",\n    [\n        (\"write pytest book\", \"done\"),\n        (\"create video course\", \"in prog\"),\n        (\"write tdd book\", \"todo\"),\n    ],\n)\ndef test_finish(cards_db, start_summary, start_state):\n    initial_card = Card(summary=start_summary, state=start_state)\n    index = cards_db.add_card(initial_card)\n\n    cards_db.finish(index)\n\n    card = cards_db.get_card(index)\n    assert card.state == \"done\"\n\n\n# simple parameters, only the start_state is varied, summary is reused.\n@pytest.mark.parametrize(\"start_state\", [\"done\", \"in prog\", \"todo\"])\ndef test_finish_simple(cards_db, start_state):\n    c = Card(\"write tdd book\", state=start_state)\n    index = cards_db.add_card(c)\n    cards_db.finish(index)\n    card = cards_db.get_card(index)\n    assert card.state == \"done\"\n\n# parameterize the fixture\n@pytest.fixture(params=[\"done\", \"in prog\", \"todo\"])\ndef start_state(request):\n    # do work\n    return request.param\n\n\ndef test_finish(cards_db, start_state):\n    c = Card(\"write tdd book\", state=start_state)\n    index = cards_db.add_card(c)\n    cards_db.finish(index)\n    card = cards_db.get_card(index)\n    assert card.state == \"done\"\n</code></pre>\n<h3>Hooks</h3>\n<p><a href=\"https://docs.pytest.org/en/7.1.x/reference/reference.html#hooks\">https://docs.pytest.org/en/7.1.x/reference/reference.html#hooks</a></p>\n<p>Normally used by plugins, can be used in conftest files.</p>\n<p>This example looks for a fixture named 'start_state' and parameterizes it.</p>\n<p><code>parametrize</code> is spelt funky.</p>\n<pre><code class=\"language-python\">def pytest_generate_tests(metafunc):\n    if \"start_state\" in metafunc.fixturenames:\n        metafunc.parametrize(\"start_state\", [\"done\", \"in prog\", \"todo\"])\n\n\ndef test_finish(cards_db, start_state):\n    c = Card(\"write tdd book\", state=start_state)\n    index = cards_db.add_card(c)\n    cards_db.finish(index)\n    card = cards_db.get_card(index)\n    assert card.state == \"done\"\n</code></pre>\n<h2>Markers</h2>\n<ul>\n<li>\n<p><code>@pytest.mark.parametrize()</code></p>\n</li>\n<li>\n<p><code>@pytest.mark.skip()</code> skip the test, use reason to give info as to why</p>\n</li>\n<li>\n<p><code>@pytest.mark.skipif()</code> conditionally skip. Supports multiple ifs.</p>\n</li>\n<li>\n<p><code>@pytest.mark.xfail()</code> exepected to fail. Can set strict=True in the ini file.</p>\n</li>\n<li>\n<p>Use <code>--markers</code> to show all markers</p>\n</li>\n</ul>\n<p>Run with <code>-ra</code> to show reasons. <code>-r</code> shows extra summary. <code>-a</code> is all except passed.</p>\n<p>Run with <code>-m</code> to run a marker, you can put logic like and or or in these</p>\n<p><code>s</code> is skipped\n<code>A</code> all</p>\n<pre><code class=\"language-python\">from packaging.version import parse\n\n# skip if the major is less than 2\n@pytest.mark.skipif(\n    parse(cards.__version__).major &#x3C; 2,\n    reason=\"Card &#x3C; comparison not supported in 1.x\",\n)\n\n# expects version less than 2 to fails\n@pytest.mark.xfail(\n    parse(cards.__version__).major &#x3C; 2,\n    reason=\"Card &#x3C; comparison not supported in 1.x\",\n)\n\n# will mark as xpass or xfail\n@pytest.mark.xfail(reason=\"XPASS demo\")\n\n# Waiting for a fix, whwen it passes alert\n@pytest.mark.xfail(reason=\"strict demo\", strict=True)\n</code></pre>\n<h3>Reasons to Use</h3>\n<ul>\n<li>skipif, rarely if ever, not for versions. OS diffs.</li>\n<li>xfail to watch for a fix from someoneelse</li>\n<li>skip hard to justify</li>\n</ul>\n<h3>Custom Markers</h3>\n<p>Register</p>\n<pre><code class=\"language-python\">[pytest]\nmarkers =\n    smoke: subset of tests\n    exception: check for expected exceptions\n</code></pre>\n<pre><code class=\"language-python\"># mark\n@pytest.mark.smoke\n\n# run with\npytest -m smoke\n</code></pre>\n<ul>\n<li><code>pytestmark = pytest.mark.finish</code> at top of file to mark all in file.</li>\n<li><code>pytest.param(\"in prog\", marks=pytest.mark.smoke)</code> mark param</li>\n<li>Can apply to function, classes, params</li>\n<li>Strict. <code>--strict-markers</code> will go faster and report mistyped markers.</li>\n</ul>\n<pre><code class=\"language-python\"># pytest.ini settings\naddopts =\n    --strict-markers\n    -ra\n</code></pre>\n<h3>Using with Fixtures</h3>\n<p><code>pip install faker</code></p>\n<pre><code class=\"language-python\">@pytest.fixture(scope=\"function\")\ndef cards_db(db, request, faker):\n    # start empty\n    db.delete_all()\n\n    # support for `@pytest.mark.num_cards(&#x3C;some number>)`\n\n    # random seed\n    faker.seed_instance(101)\n    m = request.node.get_closest_marker(\"num_cards\")\n    if m and len(m.args) > 0:\n        num_cards = m.args[0]\n        for _ in range(num_cards):\n            db.add_card(\n                Card(summary=faker.sentence(), owner=faker.first_name())\n            )\n    return db\n\ndef test_no_marker(cards_db):\n    assert cards_db.count() == 0\n\n\n@pytest.mark.num_cards\ndef test_marker_with_no_param(cards_db):\n    assert cards_db.count() == 0\n\n\n@pytest.mark.num_cards(3)\ndef test_three_cards(cards_db):\n    assert cards_db.count() == 3\n    # just for fun, let's look at the cards Faker made for us\n    print()\n    for c in cards_db.list_cards():\n        print(c)\n</code></pre>\n<h2>Testing Stategy</h2>\n<ul>\n<li>write down test strategy</li>\n<li>change strategy when needed</li>\n<li>communicate changes</li>\n</ul>\n<h3>Test Scope</h3>\n<ul>\n<li>User visiable functions / features</li>\n<li>Security</li>\n<li>Performance</li>\n<li>Loading</li>\n<li>Input Validation</li>\n<li>Testing enough to sleep at night</li>\n</ul>\n<h3>Architecture</h3>\n<ul>\n<li>how is the application divided?</li>\n<li>team divisions</li>\n<li>consider layers, subsystems, even directory structure</li>\n<li>is there an API? is there a UI?</li>\n<li>Any external service/system that needs mocked, stubbed, or otherwise avoided during testing?</li>\n</ul>\n<h3>Prioritizes</h3>\n<ul>\n<li>recent</li>\n<li>core</li>\n<li>risk</li>\n<li>problmatic</li>\n<li>expertise</li>\n</ul>\n<h3>Creating test cases</h3>\n<ol>\n<li>start with a non-trivial happy path</li>\n<li>Then look at cases that represent\n<ul>\n<li>interesting sets of input</li>\n<li>interesting starting states</li>\n<li>interesting end states</li>\n<li>all possible error states</li>\n</ul>\n</li>\n</ol>\n<h2>Configuration Files</h2>\n<ul>\n<li><code>conftest.py</code>\n<ul>\n<li>contains fixtures and hook functions</li>\n<li>can be at root or subdirectory</li>\n</ul>\n</li>\n<li><code>__init__.py</code>\n<ul>\n<li>show in test subdirectories</li>\n<li>allow test file name duplication</li>\n</ul>\n</li>\n<li><code>tox.ini</code>\n<ul>\n<li>for using tox, can combine pytest.ini here</li>\n</ul>\n</li>\n<li><code>setup.cfg</code>\n<ul>\n<li>not recommended by pytest, can cause weird issues hard to track down</li>\n</ul>\n</li>\n<li><code>pyproject.toml</code>\n<ul>\n<li>used with poetry</li>\n</ul>\n</li>\n</ul>\n<p>pytest determines root directory by starting at current and looking for conf file in each parent until found.</p>\n<ul>\n<li>recommended to add a conf file even if empty in each dir to prevent search up</li>\n</ul>\n<p>Example files</p>\n<pre><code class=\"language-python\"># pytest.ini\n[pytest]\naddopts =\n    --strict-markers\n    --strict-config\n    -ra\n\ntestpaths = tests\n\nmarkers =\n    smoke: subset of tests\n    exception: check for expected exceptions\n\n# tox.ini, can combine pytest.init setting\n[tox]\n; tox specific settings\n\n[pytest]\naddopts =\n    --strict-markers\n    --strict-config\n    -ra\n\ntestpaths = tests\n\nmarkers =\n    smoke: subset of tests\n    exception: check for expected exceptions\n\n# setup.cfg, least recommended\n[tool:pytest]\naddopts =\n    --strict-markers\n    --strict-config\n    -ra\n\ntestpaths = tests\n\nmarkers =\n    smoke: subset of tests\n    exception: check for expected exceptions\n\n# pyproject.toml\n[tool.pytest.ini_options]\naddopts = [\n    \"--strict-markers\",\n    \"--strict-config\",\n    \"-ra\"\n    ]\n\ntestpaths = \"tests\"\n\nmarkers = [\n\t\"smoke: subset of tests\",\n\t\"exception: check for expected exceptions\"\n]\n</code></pre>\n<h3>Avoid Name Collision</h3>\n<p><code>__init__.py</code> with pytest will avoid name collisions, the file can be empty. Doesn't affect imports.</p>\n<p>Recommended to always add to sub directories.</p>\n<h2>Coverage</h2>\n<p><code>pip install pytest-cov</code> also installs coverage.</p>\n<p><code>pytest ch07 --cov=cards</code> run coverage for a directory for an installed package</p>\n<p><code>coverage run --source=cards -m pytest ch07</code> runs with coverage directly</p>\n<p><code>coverage report</code> runs the report separate</p>\n<p><code>coverage report --show-missing</code> show missing</p>\n<p><code>pytest --cov=cards ch07 --cov-report=term-missing</code> run missing with pytest</p>\n<p>.coveragerc</p>\n<pre><code>[paths]\nsource =\n  cards_proj/src/cards\n  */site-packages/cards\n</code></pre>\n<p>This will map site-packages path to the local dir for better output</p>\n<p><code>pytest --cov=cards ch07 --cov-report=html</code> generate html, or run as normal and then run <code>coverage html</code></p>\n<p><code>--cov-branch</code> will collect branch coverage</p>\n<p><code>#pragma: no cover</code> skip coverage for a line, will show gray in report</p>\n<p><code>pytest ch09/tests_api -cov=cards --cov=ch09/tests_api</code> will get coverage on tests as well. This will catch test name collisions which drop coverage.</p>\n<p><code>--cov=single_file</code> will execute for a single file</p>\n<h2>Mocking</h2>\n<p><code>typer</code> running and testing CLIs\n<code>shlex</code> utility for parsing command line args</p>\n<h3>patch attribute</h3>\n<pre><code class=\"language-python\"># patch cards, __version__ and return 1.2.3\ndef test_mock_version():\n    with mock.patch.object(cards, \"__version__\", \"1.2.3\"):\n        result = cards_cli(\"version\")\n        assert result == \"1.2.3\"\n</code></pre>\n<h3>Patch Class and Methods</h3>\n<pre><code class=\"language-python\">from unittest import mock\n\nimport cards\n\nfrom cards_cli_helper import cards_cli\n\n# patch the cards.CardsDB\ndef test_config():\n    with mock.patch.object(cards, \"CardsDB\") as MockCardsDB:\n        MockCardsDB.return_value.path.return_value = \"/foo/\"\n        result = cards_cli(\"config\")\n        assert result == \"/foo/\"\n</code></pre>\n<h3>Fixture for Mocking</h3>\n<p>The fixture patches the mock source.</p>\n<pre><code class=\"language-python\">from unittest import mock\n\nimport cards\nimport pytest\n\nfrom cards_cli_helper import cards_cli\n\n\n@pytest.fixture()\ndef mock_cardsdb():\n    with mock.patch.object(cards, \"CardsDB\") as CardsDB:\n        yield CardsDB.return_value\n\n\ndef test_config(mock_cardsdb):\n    mock_cardsdb.path.return_value = \"/foo/\"\n    result = cards_cli(\"config\")\n    assert result == \"/foo/\"\n\n</code></pre>\n<h3>Autospec</h3>\n<p><code>autospec=True</code> is safer to use with mocks.</p>\n<pre><code class=\"language-python\">from unittest import mock\n\nimport cards\nimport pytest\n\nfrom cards_cli_helper import cards_cli\n\n\n@pytest.fixture()\ndef mock_cardsdb():\n    with mock.patch.object(cards, \"CardsDB\") as CardsDB:\n        yield CardsDB.return_value\n\n# passes but invalid\ndef test_bad_mock(mock_cardsdb):\n    mock_cardsdb.path(35)  # invalid arguments\n    mock_cardsdb.not_valid()  # invalid function\n\n# would catch bad interface\n@pytest.fixture()\ndef mock_cardsdb_with_autospec():\n    with mock.patch.object(cards, \"CardsDB\", autospec=True) as CardsDB:\n        yield CardsDB.return_value\n</code></pre>\n<h3>Asserting Calls</h3>\n<pre><code class=\"language-python\">def test_start(mock_cardsdb):\n    cards_cli(\"start 23\")\n    mock_cardsdb.start.assert_called_with(23)\n</code></pre>\n<h3>Exceptions</h3>\n<p>Testing exceptions uses side_effect</p>\n<pre><code class=\"language-python\">def test_delete_invalid(mock_cardsdb):\n    mock_cardsdb.delete_card.side_effect = cards.api.InvalidCardId\n    out = cards_cli(\"delete 25\")\n    assert \"Error: Invalid card id 25\" in out\n\n\ndef test_start_invalid(mock_cardsdb):\n    mock_cardsdb.start.side_effect = cards.api.InvalidCardId\n    out = cards_cli(\"start 25\")\n    assert \"Error: Invalid card id 25\" in out\n</code></pre>\n<h2>Exclusions</h2>\n<p>.coveragerc exclude func name <strong>main</strong>, this function is just for testing manually.</p>\n<pre><code class=\"language-python\">[report]\nexclude_also =\n  if __name__ == .__main__.:\n</code></pre>\n<h2>Details</h2>\n<p><a href=\"https://docs.python.org/3/library/unittest.mock.html#module-unittest.mock\">https://docs.python.org/3/library/unittest.mock.html#module-unittest.mock</a></p>\n<p>Downsides</p>\n<ul>\n<li>tests with mocks don't test behavior</li>\n<li>test with mocks test implementation</li>\n<li>refactoring can cause mock-based tests to fail</li>\n<li>These are bad kind of change detector test\n<ul>\n<li>detecting behavior changes -> good</li>\n<li>detecting implementation change -> bad</li>\n</ul>\n</li>\n</ul>\n<p>When to Use</p>\n<ul>\n<li>exceptions or error conditions, difficult to reproduce</li>\n<li>external system that are slow or unreasonable to use with testing\n<ul>\n<li>payment gateways</li>\n<li>sending email</li>\n</ul>\n</li>\n<li>it's nice to have mocks but don't reach for them first</li>\n</ul>\n<h2>Avoiding Mocks</h2>\n<p>Test at multiple layers.</p>\n<ul>\n<li>Using other layers to execute another test will improve coverage and testing, this is preferred to mocks</li>\n</ul>\n<h2>Plugins</h2>\n<ul>\n<li>databases\n<ul>\n<li>pytest-postgresql, pytest-mongo, pytest-mysql, pytest-dynamodb</li>\n</ul>\n</li>\n<li>http servers\n<ul>\n<li>pytest-httpserver</li>\n</ul>\n</li>\n<li>requests\n<ul>\n<li>responses, betamax</li>\n</ul>\n</li>\n<li>misc\n<ul>\n<li>pytest-rabbitmq, pytest-redis</li>\n</ul>\n</li>\n</ul>\n<h2>tox</h2>\n<ul>\n<li>mini local ci system</li>\n<li>build project, create venv, install projects and dependencies, run tests</li>\n<li>can run multiple versions of python</li>\n<li>can have a matrix of dependent project versions</li>\n<li>running linters and other checks</li>\n<li>helping determine if you code is ready to push</li>\n<li>later runs are faster</li>\n</ul>\n<p><code>pip install tox</code></p>\n<p>tox.ini</p>\n<pre><code class=\"language-ini\">[tox]\nenvlist = py312\n\n[testenv]\ndeps = pytest\ncommands = pytest\n\n</code></pre>\n<p>Run with <code>tox</code></p>\n<p>Run with a config <code>tox -c tox_2_mult_pythons.ini</code></p>\n<p>Run one env with <code>-e py312</code></p>\n<p>multiple pythons</p>\n<pre><code class=\"language-ini\">[tox]\nenvlist = py38, py39, py310, py311, py312\n\n[testenv]\ndeps = pytest\ncommands = pytest\n\n; package\n;\n; tells tox what kind of thing to build\npackage = wheel\n\n; wheel_build_env\n;\n; which environment to save the built wheel\n; when set to a constant, tells tox to\n; only build the wheel once for all environments\n;\n; In our case, it will show up in .tox/.pkg\nwheel_build_env = .pkg\n</code></pre>\n<p>with coverage</p>\n<pre><code class=\"language-ini\">[tox]\nenvlist = py38, py39, py310, py311, py312\n\n[testenv]\ndeps =\n  pytest\n  pytest-cov\ncommands = pytest --cov=cards --cov=tests\npackage = wheel\nwheel_build_env = .pkg\n</code></pre>\n<p>coverage fail below a percentage: <code>--cov-fail-under=100</code></p>\n<p>positional args</p>\n<pre><code class=\"language-ini\">commands =\n  pytest --cov=cards --cov=tests --cov-branch --cov-fail-under=100 {posargs}\n</code></pre>\n<p><code>tox -c tox_5_posargs.ini -e py312 -- -v -k test_version --no-cov</code></p>\n<p>final config</p>\n<pre><code class=\"language-ini\">[tox]\nenvlist = py38, py39, py310, py311, py312\nskip_missing_interpreters = True\n\n[testenv]\ndeps =\n  pytest\n  pytest-cov\ncommands =\n  pytest --cov=cards --cov=tests {posargs}\npackage = wheel\nwheel_build_env = .pkg\n\n[pytest]\naddopts =\n    --strict-markers\n    --strict-config\n    -ra\n    --cov-branch\n    --cov-fail-under=100\n    --color=yes\ntestpaths = tests\npythonpath = tests/cli\n\n[coverage:paths]\nsource =\n    src\n    */site-packages\"\n\n[coverage:report]\nexclude_also =\n    if __name__ == .__main__.:\n</code></pre>\n<p><code>skip_missing_interpreters</code> will allow it to run if only some versions are on a machine, needs one</p>\n<p>combines pytest.ini and tox.ini</p>\n<h3>GitHub Actions</h3>\n<p><a href=\"https://docs.github.com/en/actions/automating-builds-and-tests/building-and-testing-python\">https://docs.github.com/en/actions/automating-builds-and-tests/building-and-testing-python</a></p>\n<pre><code>name: main\n\non: [push, pull_request]\n\nenv:\n  PYTHONUNBUFFERED: \"1\"\n  FORCE_COLOR: \"1\"\n\njobs:\n  build:\n\n    runs-on: ubuntu-latest\n\n    strategy:\n      matrix:\n        python: [\"3.8\", \"3.9\", \"3.10\", \"3.11\", \"3.12\"]\n\n    steps:\n      - uses: actions/checkout@v4\n      - name: Setup Python\n        uses: actions/setup-python@v4\n        with:\n          python-version: ${{ matrix.python }}\n          cache: 'pip'\n      - name: Install Tox and any other packages\n        run: pip install tox\n      - name: Run Tox\n        run: tox -e py\n\n</code></pre>\n<p><code>PYTHONUNBUFFERED</code> don't buffer python output, print right away. Helpful for logging.\n<code>FORCE_COLOR</code> tox colors</p>\n<p>Runs tox for each version.</p>\n<p><code>tox -e py</code> runs tox for the installed version</p>\n<h2>Testing Scripts and Applications</h2>\n<h3>Scripts</h3>\n<pre><code class=\"language-python\"># source\nimport argparse\n\nparser = argparse.ArgumentParser()\nparser.add_argument(\"name\", type=str, default=\"World\", nargs=\"?\")\nargs = parser.parse_args()\n\nprint(f\"Hello, {args.name}!\")\n</code></pre>\n<pre><code class=\"language-python\">def test_hello():\n    result = run([\"python\", \"hello_world.py\"], capture_output=True, text=True)\n    output = result.stdout\n    assert output == \"Hello, World!\\n\"\n</code></pre>\n<p>Args</p>\n<pre><code class=\"language-python\"># without helpers\ndef test_hello_with_name():\n    result = run([\"python\", \"hello.py\", \"Brian\"], capture_output=True, text=True)\n    output = result.stdout\n    assert output == \"Hello, Brian!\\n\"\n\n# with helpers\ndef test_hello():\n    command = shlex.split(\"python hello.py\")\n    result = run(command, capture_output=True, text=True)\n    output = result.stdout\n    assert output == \"Hello, World!\\n\"\n\n\ndef test_hello_with_name():\n    command = shlex.split(\"python hello.py Brian\")\n    result = run(command, capture_output=True, text=True)\n    output = result.stdout\n    assert output == \"Hello, Brian!\\n\"\n\n# with params and helpers\n@pytest.mark.parametrize(\n    \"command, expected\",\n    [\n        (\"python hello.py\", \"Hello, World!\"),\n        (\"python hello.py Brian\", \"Hello, Brian!\"),\n    ],\n)\ndef test_hello(command, expected):\n    result = run(shlex.split(command), capture_output=True, text=True)\n    output = result.stdout.rstrip()\n    assert output == expected\n</code></pre>\n<p>tox</p>\n<pre><code class=\"language-ini\">[tox]\nenvlist = py311, py312\nno_package = true\n\n[testenv]\ndeps = pytest\ncommands = pytest\n\n[pytest]\n</code></pre>\n<h3>script with functions</h3>\n<pre><code class=\"language-python\"># source\nimport argparse\n\n\ndef parse_args(arg_list: list[str] | None):\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"name\", type=str, default=\"World\", nargs=\"?\")\n    return parser.parse_args(arg_list)\n\n\ndef main(arg_list: list[str] | None = None):\n    args = parse_args(arg_list)\n    print(f\"Hello, {args.name}!\")\n\n\nif __name__ == \"__main__\":\n    main()\n</code></pre>\n<p>biggest difference is imports and calls</p>\n<pre><code class=\"language-python\">import hello\n\n\ndef test_hello(capsys):\n    hello.main([])\n    output = capsys.readouterr().out.rstrip()\n    assert output == \"Hello, World!\"\n\n\ndef test_hello_brian(capsys):\n    hello.main([\"Brian\"])\n    output = capsys.readouterr().out.rstrip()\n    assert output == \"Hello, Brian!\"\n\n\ndef test_parse_args_no_args():\n    arg_n = hello.parse_args([])\n    assert arg_n.name == \"World\"\n\n\ndef test_parse_args_Brian():\n    arg_n = hello.parse_args([\"Brian\"])\n    assert arg_n.name == \"Brian\"\n</code></pre>\n<h3>Splitting src and test</h3>\n<p>This tells python where src are tests are.</p>\n<pre><code class=\"language-ini\">[pytest]\npythonpath = src\ntestpaths = tests\n</code></pre>\n<h3>Apps</h3>\n<ul>\n<li>requirements.txt can be pinned or unpinned, tox will use it</li>\n</ul>\n<pre><code class=\"language-text\">click==8.1.7\n</code></pre>\n<pre><code class=\"language-ini\">[tox]\nenvlist = py311, py312\nno_package = true\n\n[testenv]\ndeps = pytest\n       -rrequirements.txt\ncommands = pytest\n\n[pytest]\ntestpaths = tests\npythonpath = src\n</code></pre>\n<pre><code class=\"language-python\"># source\nimport click\n\n\n@click.command()\n@click.argument(\"name\", required=False, default=\"World\")\ndef main(name: str):\n    print(f\"Hello, {name}!\")\n\n\nif __name__ == \"__main__\":\n    main()\n</code></pre>\n<pre><code class=\"language-python\">import hello\nfrom click.testing import CliRunner\n\nrunner = CliRunner()\n\n\ndef test_hello():\n    result = runner.invoke(hello.main)\n    assert result.stdout.rstrip() == \"Hello, World!\"\n\n\ndef test_hello_with_name():\n    result = runner.invoke(hello.main, [\"Brian\"])\n    assert result.stdout.rstrip() == \"Hello, Brian!\"\n\n</code></pre>\n<h2>Errors and Debugging</h2>\n<p><code>pip install -e ./cards_proj</code> install as editable</p>\n<pre><code class=\"language-toml\">[project.optional-dependencies]\ntest = [\n    \"pytest\",\n    \"tox\",\n    \"coverage\",\n    \"pytest-cov\",\n]\n</code></pre>\n<p>Install the test group: <code>pip install -e \"./cards_proj[test]\"</code></p>\n<h2>Debugging with pytest flags</h2>\n<h3>Flags for selecting which tests to run, in which order, and when to stop:</h3>\n<ul>\n<li><code>--lf</code> / <code>--last-failed</code>: Runs just the tests that failed last</li>\n<li><code>--ff</code> / <code>--failed-first</code>: Runs all the tests, starting with the last failed</li>\n<li><code>-x</code> / <code>--exitfirst</code>: Stops the tests session after the first failure</li>\n<li><code>--maxfail=num</code>: Stops the tests after num failures</li>\n<li><code>--nf</code> / <code>--new-first</code>: Runs all the tests, ordered by file modification time</li>\n<li><code>--sw</code> / <code>--stepwise</code>: Stops the tests at the first failure. Starts the tests at the last failure next time</li>\n<li><code>--sw-skip</code> / <code>--stepwise-skip</code>: Same as --sw, but skips the first failure</li>\n</ul>\n<h3>Flags to control pytest output:</h3>\n<ul>\n<li><code>-v</code> / <code>--verbose</code>: Displays all the test names, passing or failing</li>\n<li><code>--tb=[auto/long/short/line/native/no]</code>: Controls the traceback style</li>\n<li><code>-l</code> / <code>--showlocals</code>: Displays local variables alongside the stacktrace</li>\n</ul>\n<h3>Flags to start a command-line debugger:</h3>\n<ul>\n<li><code>--pdb</code>: Starts an interactive debugging session at the point of failure</li>\n<li><code>--trace</code>: Starts the pdb source-code debugger immediately when running each test</li>\n</ul>\n<h2>Debugging with pdb</h2>\n<p>Where to get more help:</p>\n<ul>\n<li><a href=\"https://docs.python.org/3/library/pdb.html\">https://docs.python.org/3/library/pdb.html</a></li>\n<li><a href=\"https://docs.python.org/3/library/pdb.html#debugger-commands\">https://docs.python.org/3/library/pdb.html#debugger-commands</a></li>\n</ul>\n<h3>Meta commands:</h3>\n<ul>\n<li><code>h(elp)</code>: Prints a list of commands</li>\n<li><code>h(elp) command</code>: Prints help on a command</li>\n<li><code>q(uit)</code>: Exits pdb</li>\n</ul>\n<h3>Seeing where you are:</h3>\n<ul>\n<li><code>l(ist)</code>: Lists 11 lines around the current line. Using it again lists the next 11 lines, and so on.</li>\n<li><code>l(ist) .</code>: The same as above, but with a dot. Lists 11 lines around the current line. Handy if you’ve use l(list) a few times and have lost your current position</li>\n<li><code>l(ist) first, last</code>: Lists a specific set of lines</li>\n<li><code>ll</code>: Lists all source code for the current function</li>\n<li><code>w(here)</code>: Prints the stack trace</li>\n</ul>\n<h3>Looking at values:</h3>\n<ul>\n<li><code>p(rint) expr</code>: Evaluates expr and prints the value</li>\n<li><code>pp expr</code>: Same as p(rint) expr but uses pretty-print from the pprint module.</li>\n</ul>\n<h3>Great for structures</h3>\n<ul>\n<li><code>a(rgs)</code>: Prints the argument list of the current function</li>\n</ul>\n<h3>Execution commands:</h3>\n<ul>\n<li><code>s(tep)</code>: Executes the current line and steps to the next line in your source code even if it’s inside a function</li>\n<li><code>n(ext)</code>: Executes the current line and steps to the next line in the current</li>\n</ul>\n<p>function</p>\n<ul>\n<li><code>r(eturn)</code>: Continues until the current function returns</li>\n<li><code>c(ontinue)</code>: Continues until the next breakpoint. When used with --trace, continues until the start of the next test</li>\n<li><code>unt(il) lineno</code>: Continues until the given line number</li>\n</ul>\n<h3>Debugging With Tox</h3>\n<p><code>tox -c cards_proj/tox.ini -e py312 -- --pdb</code> pass in pdb</p>\n<h2>Plugins</h2>\n<ul>\n<li><a href=\"https://docs.pytest.org/en/latest/reference/plugin_list.html\">https://docs.pytest.org/en/latest/reference/plugin_list.html</a></li>\n<li><a href=\"https://pypi.org/\">https://pypi.org/</a> search <code>pytest-</code></li>\n<li><a href=\"https://github.com/pytest-dev\">https://github.com/pytest-dev</a></li>\n<li><a href=\"https://docs.pytest.org/en/latest/how-to/plugins.html\">https://docs.pytest.org/en/latest/how-to/plugins.html</a></li>\n<li><a href=\"https://podcast.pythontest.com/\">https://podcast.pythontest.com/</a></li>\n</ul>\n<h3>Change Flow</h3>\n<ul>\n<li>pytest-order specify order with markers</li>\n<li>pytest-randomly randomize test runs. Use <code>-p no:randomly</code> to turn off.</li>\n<li>pytest-repeat repeat single or multiple test</li>\n<li>pytest-rerunfailures only rerun failures</li>\n<li>pytest-xdist run in parallel</li>\n</ul>\n<h3>Change Output</h3>\n<ul>\n<li>pytest-instafail fail with full output instantly. normal output is saved up</li>\n<li>pytest-sugar change dots to checks</li>\n<li>pytest-html gen html report</li>\n</ul>\n<h3>Web Development</h3>\n<ul>\n<li>pytest-selenium selenium driver</li>\n<li>pytest-splinter nicer wrapper for selenium</li>\n<li>pytest-django for testing django</li>\n<li>pytest-flask for testing flask</li>\n</ul>\n<h3>Fake Data</h3>\n<ul>\n<li>faker generate fake data</li>\n<li>model-bakery generate djando objects with fake data</li>\n<li>pytest-factoryboy generate db models for testing</li>\n<li>pytest-mimesis like faker but supposedly faster</li>\n</ul>\n<h3>Extend pytest</h3>\n<ul>\n<li>pytest-cov output coverage</li>\n<li>pytest-benchmark timing</li>\n<li>pytest-timeout make sure they don't run too long</li>\n<li>pytest-asyncio testing async</li>\n<li>pytest-bdd test in bdd style</li>\n<li>pytest-freezegun freezes time</li>\n<li>pytest-mock wrapper around unit test mock api</li>\n</ul>\n<h2>Plugin</h2>\n<p>Local plugin to skip slow tests</p>\n<pre><code class=\"language-ini\">import pytest\n\n\ndef pytest_configure(config):\n    config.addinivalue_line(\"markers\", \"slow: mark test as slow to run\")\n\n\n\ndef pytest_addoption(parser):\n    parser.addoption(\n        \"--slow\", action=\"store_true\", help=\"include tests marked slow\"\n    )\n\n\ndef pytest_collection_modifyitems(config, items):\n    if config.getoption(\"--slow\"):\n        # go ahead and run tests as normal\n        return\n    else:\n        # skip the slow tests\n        skip_slow = pytest.mark.skip(reason=\"need --slow option to run\")\n        for item in items:\n            if item.get_closest_marker(\"slow\"):\n                item.add_marker(skip_slow)\n</code></pre>\n<h2>Building a Plugin</h2>\n<ul>\n<li><a href=\"https://packaging.python.org/en/latest/\">https://packaging.python.org/en/latest/</a></li>\n<li><a href=\"https://podcast.pythontest.com/episodes/197-python-project-trove-classifiers-do-you-need-this-bit-of-pyproject-toml-metadata\">https://podcast.pythontest.com/episodes/197-python-project-trove-classifiers-do-you-need-this-bit-of-pyproject-toml-metadata</a></li>\n<li><a href=\"https://docs.pytest.org/en/latest/reference/reference.html#pytest.RunResult\">https://docs.pytest.org/en/latest/reference/reference.html#pytest.RunResult</a></li>\n</ul>\n<ol>\n<li><code>pip intall flit</code></li>\n<li><code>flit init</code></li>\n<li>Edit <code>pyproject.toml</code></li>\n</ol>\n<pre><code class=\"language-toml\">[build-system]\nrequires = [\"flit_core >=3.2,&#x3C;4\"]\nbuild-backend = \"flit_core.buildapi\"\n\n[project]\nname = \"pytest-skip-slow\"\nauthors = [{name = \"A Person\"}]\nlicense = {file = \"LICENSE\"}\nclassifiers = [\"License :: OSI Approved :: MIT License\", \"Framework :: Pytest\"]\n# dynamic = [\"version\", \"description\"]\nversion = \"0.0.1\"\ndescription = \"Skip slow tests with Pytest\"\nreadme = \"README.md\"\n\n[project.urls]\nHome = \"https://github.com/okken/pytest_skip_slow\"\n\n[project.entry-points.pytest11]\nskip_slow = \"pytest_skip_slow\"\n\n[tool.flit.module]\nname = \"pytest_skip_slow\"\n</code></pre>\n<p>Run <code>flit build</code> or <code>flit build --no-use-vcs</code></p>\n<ol>\n<li><code>pip install pytest</code></li>\n<li><code>pip install -e ./</code></li>\n<li><code>pytest examples</code></li>\n</ol>\n<h3>Testing</h3>\n<pre><code class=\"language-toml\">[project.optional-dependencies]\ntest = [\"tox\"]\n</code></pre>\n<p>tox.ini. This will build once for all envs.</p>\n<pre><code class=\"language-ini\">[tox]\nenvlist = py{38, 38, 39, 310, 311, 312}\n\n[testenv]\ndeps = pytest\npackage = wheel\nwheel_build_env = .pkg\ncommands = pytest {posargs}\ndescription = Run pytest\n\n[pytest]\ntestpaths = tests\n</code></pre>\n<p>conftest.py</p>\n<pre><code class=\"language-python\">pytest_plugins = [\"pytester\"]\n</code></pre>\n<p>example test</p>\n<pre><code class=\"language-python\">\n@pytest.fixture()\ndef examples(pytester):\n    pytester.copy_example(\"examples/test_slow.py\")\n\n\ndef test_skip_slow(pytester, examples):\n    result = pytester.runpytest(\"-v\")\n    result.stdout.fnmatch_lines(\n        [\n            \"*test_normal PASSED*\",\n            \"*test_slow SKIPPED (need --slow option to run)*\",\n        ]\n    )\n    result.assert_outcomes(passed=1, skipped=1)\n\n\ndef test_run_slow(pytester, examples):\n    result = pytester.runpytest(\"--slow\")\n    result.assert_outcomes(passed=2)\n\n\ndef test_run_only_slow(pytester, examples):\n    result = pytester.runpytest(\"-v\", \"-m\", \"slow\", \"--slow\")\n    result.stdout.fnmatch_lines([\"*test_slow PASSED*\"])\n    outcomes = result.parseoutcomes()\n    assert outcomes[\"passed\"] == 1\n    assert outcomes[\"deselected\"] == 1\n\n\ndef test_help(pytester):\n    result = pytester.runpytest(\"--help\")\n    result.stdout.fnmatch_lines(\n        [\"*--slow * include tests marked slow*\"]\n    )\n</code></pre>\n<h3>Using src Directory</h3>\n<p>Point to the plugin entrypoint and add <strong>init</strong>.py in the module.</p>\n<pre><code class=\"language-toml\">[project.entry-points.pytest11]\nskip_slow = \"pytest_skip_slow.plugin\"\n</code></pre>\n<h2>Advanced Parametrization</h2>\n<h3>Using custom identifiers ids option in <code>@pytest.mark.parametrize(ids=...)</code></h3>\n<pre><code class=\"language-python\"># return default str result\ncard_list = [\n    Card(\"foo\", state=\"todo\"),\n    Card(\"foo\", state=\"in prog\"),\n    Card(\"foo\", state=\"done\"),\n]\n\n@pytest.mark.parametrize(\"starting_card\", card_list, ids=str)\ndef test_id_str(cards_db, starting_card):\n    index = cards_db.add_card(starting_card)\n    cards_db.finish(index)\n    card = cards_db.get_card(index)\n    assert card.state == \"done\"\n\n# return by status\ndef card_state(card):\n    return card.state\n\n\n@pytest.mark.parametrize(\"starting_card\", card_list, ids=card_state)\ndef test_id_func(cards_db, starting_card):\n    index = cards_db.add_card(starting_card)\n    cards_db.finish(index)\n    card = cards_db.get_card(index)\n    assert card.state == \"done\"\n\n# or\n@pytest.mark.parametrize( \"starting_card\", card_list, ids=lambda c: c.state)\ndef test_id_lambda(cards_db, starting_card):\n    ...\n    index = cards_db.add_card(starting_card)\n    cards_db.finish(index)\n    card = cards_db.get_card(index)\n    assert card.state == \"done\"\n\n# override\nc_list = [\n    Card(\"foo\", state=\"todo\"),\n    pytest.param(Card(\"foo\", state=\"in prog\"), id=\"special\"),\n    Card(\"foo\", state=\"done\"),\n]\n\n\n@pytest.mark.parametrize(\"starting_card\", c_list, ids=card_state)\ndef test_id_param(cards_db, starting_card):\n    ...\n    index = cards_db.add_card(starting_card)\n    cards_db.finish(index)\n    card = cards_db.get_card(index)\n    assert card.state == \"done\"\n\n\n# list\nid_list = [\"todo\", \"in prog\", \"done\"]\n\n\n@pytest.mark.parametrize(\"starting_card\", card_list, ids=id_list)\ndef test_id_list(cards_db, starting_card):\n    ...\n    index = cards_db.add_card(starting_card)\n    cards_db.finish(index)\n    card = cards_db.get_card(index)\n    assert card.state == \"done\"\n\n# with object\ntext_variants = {\n    \"Short\": \"x\",\n    \"With Spaces\": \"x y z\",\n    \"End In Spaces\": \"x    \",\n    \"Mixed Case\": \"SuMmArY wItH MiXeD cAsE\",\n    \"Unicode\": \"¡¢£¤¥¦§¨©ª«¬®¯°±²³´µ¶·¸¹º»¼½¾\",\n    \"Newlines\": \"a\\nb\\nc\",\n    \"Tabs\": \"a\\tb\\tc\",\n}\n\n\n@pytest.mark.parametrize(\n    \"variant\", text_variants.values(), ids=text_variants.keys()\n)\ndef test_summary_variants(cards_db, variant):\n    i = cards_db.add_card(Card(summary=variant))\n    c = cards_db.get_card(i)\n    assert c.summary == variant\n</code></pre>\n<h3>Using dynamic values. Generating parameters at runtime</h3>\n<p>This may affect memory, it's during test discovery.</p>\n<pre><code class=\"language-python\">def text_variants():\n    variants = {\n        \"Short\": \"x\",\n        \"With Spaces\": \"x y z\",\n        \"End in Spaces\": \"x    \",\n        \"Mixed Case\": \"SuMmArY wItH MiXeD cAsE\",\n        \"Unicode\": \"¡¢£¤¥¦§¨©ª«¬®¯°±²³´µ¶·¸¹º»¼½¾\",\n        \"Newlines\": \"a\\nb\\nc\",\n        \"Tabs\": \"a\\tb\\tc\",\n    }\n    for key, value in variants.items():\n        yield pytest.param(value, id=key)\n\n\n@pytest.mark.parametrize(\"variant\", text_variants())\ndef test_summary(cards_db, variant):\n    i = cards_db.add_card(Card(summary=variant))\n    c = cards_db.get_card(i)\n    assert c.summary == variant\n\n</code></pre>\n<h3>Using multiple parameters</h3>\n<ul>\n<li>Usually list on outside and tuple on inside.</li>\n<li>Using underscore to split words in values is preferred because output uses spaces and hyphen</li>\n</ul>\n<pre><code class=\"language-python\">@pytest.mark.parametrize(\n    \"summary, owner, state\",\n    [\n        (\"short\", \"First\", \"todo\"),\n        (\"short\", \"First\", \"in prog\"),\n        # ...\n    ],\n)\ndef test_add_lots(cards_db, summary, owner, state):\n    \"\"\"Make sure adding to db doesn't change values.\"\"\"\n    i = cards_db.add_card(Card(summary, owner=owner, state=state))\n    card = cards_db.get_card(i)\n\n    expected = Card(summary, owner=owner, state=state)\n    assert card == expected\n\n\nsummaries = [\"short\", \"a bit longer\"]\nowners = [\"First\", \"First M. Last\"]\nstates = [\"todo\", \"in prog\", \"done\"]\n\n\n@pytest.mark.parametrize(\"state\", states)\n@pytest.mark.parametrize(\"owner\", owners)\n@pytest.mark.parametrize(\"summary\", summaries)\ndef test_stacking(cards_db, summary, owner, state):\n    \"\"\"Make sure adding to db doesn't change values.\"\"\"\n    expected = Card(summary, owner=owner, state=state)\n    i = cards_db.add_card(Card(summary, owner=owner, state=state))\n    card = cards_db.get_card(i)\n    assert card == expected\n</code></pre>\n<h3>Indirect parametrization (and optionally indirect)</h3>\n<p>Looks for a fixture called 'user'</p>\n<pre><code class=\"language-python\">@pytest.fixture()\ndef user(request):\n    role = request.param\n    print(f\"\\nLog in as {role}\")\n    yield role\n    print(f\"\\nLog out {role}\")\n\n\n@pytest.mark.parametrize(\n    \"user\", [\"admin\", \"team_member\", \"visitor\"], indirect=[\"user\"]\n)\ndef test_access_rights(user):\n    print(f\"Test access rights for {user}\")\n\n\n\n@pytest.mark.parametrize(\n    \"user\", [\"admin\", \"team_member\"], indirect=[\"user\"]\n)\ndef test_special_rights(user):\n    print(f\"Test access rights for {user}\")#\n</code></pre>\n<p>selecting a subset of fixture parameters</p>\n<pre><code class=\"language-python\">@pytest.fixture(params=[\"admin\", \"team_member\", \"visitor\"])\ndef user(request):\n    role = request.param\n    print(f\"\\nLog in as {role}\")\n    yield role\n    print(f\"\\nLog out {role}\")\n\n\ndef test_everyone(user):\n    ...\n\n\n@pytest.mark.parametrize(\"user\", [\"admin\"], indirect=[\"user\"])\ndef test_just_admin(user):\n    ...\n</code></pre>\n<p>optional indirect</p>\n<p>sets a default of 'visitor'</p>\n<pre><code class=\"language-python\">@pytest.fixture()\ndef user(request):\n    role = getattr(request, \"param\", \"visitor\")\n    print(f\"\\nLog in as {role}\")\n    yield role\n    print(f\"\\nLog out {role}\")\n\n\ndef test_unspecified_user(user):\n    ...\n\n\n@pytest.mark.parametrize(\n    \"user\", [\"admin\", \"team_member\"], indirect=[\"user\"]\n)\ndef test_admin_and_team_member(user):\n    ...\n\n</code></pre>","frontmatter":{"date":"April 23, 2024","title":"Python Test Course","tags":["course","python","teachable"]}}},"pageContext":{"slug":"/workshops/pythontest/"}},"staticQueryHashes":["1796642556","2356112386","3489759178"],"slicesMap":{}}